# Semantic-content-recommendation-system-with-Amazon-SageMaker
Built a semantic, content recommendation system that combines topic modeling and nearest neighbor techniques for information retrieval using Amazon SageMaker built-in algorithms for Neural Topic Model (NTM) and K-Nearest Neighbor (K-NN).



## Overview
"Information retrieval is the science of searching for information in a document, searching for documents themselves, or searching for metadata that describe data. This combines the techniques of topic modeling and nearest neighbor for information retrieval. This approach uses topic modeling to generate semantic distribution vectors representing the meaning of documents by topics, and then uses the nearest neighbor technique to index topic vectors to retrieve similar documents for a given input document based on topic similarity. By using Amazon SageMaker built-in algorithms, you do not need to label data and the information retrieval is based on semantic meaning similarity instead of simple string matching."

![image](https://user-images.githubusercontent.com/106786020/214730815-3beaae43-9960-445d-837a-07da83ae1072.png)



## Basic initial setup 
To start things off I just made a basic S3 bucket and configured all the settings for it. Then started off by making a SageMaker Notebook instance, this was my first time making a notebook instance in the almost five years that I've been using AWS. Everything was pretty straight forward it was almost like creating any regular instance, the only thing that caught my eye is that the notebook instance has a preconfigured Jupyter notebook server and a set of Anaconda libraries. I then created a new Jupyter notebook in JupyterLab and saved.



## Download and prepare dataset
I started by fetching the dataset which didn't take to long. I just imported some libraries and defined a few environment variables in my notebook environment (Fetch data set code file). Then I started by setting up the reproccessing data code, to process raw text data into machine readable numeric values. First, I used the APIs provided by scikit-learn to strip any headers, footers and quotes from the dataset. Then I defined the code in my Jupyter notebook (strip headers, footers, and quotes file). After seeing the data was simply play text paragraphs after running the code. In order for this to be machine readable, I needed to "tokenize” this data to numeric format by assigning a “token” to each word in the sentence. Then I limited the total number of tokens to 2000 by first counting the most frequent tokens and only retaining the top 2000. "This limiting is in place because less frequent words will have a diminishing impact on the topic model and can be ignored. Then, for each of the documents you use a Bag of Words (BoW) model to convert the document into a vector which keeps track of the number of times each token appears in that training example.".


Then I used WordNetLemmatizer, a lemmatizer from the nltk package, and used CountVectorizer in scikit-learn to perform the token counting. WordNetLemmatizer uses nouns as the parts of speech (POS) for lemmatizing words into lemmas. Lemmatization aims to return actual words whereas stemming, another preprocessing approach, can often return non-dictionary words or root stems which are often less useful in machine learning. In the list comprehension, I implemented a simple rule: only consider words that are longer than 2 characters, start with a letter and match the token_pattern (tokenizer define, token counting, limiting the vocab_size to 2000 file.). Then imported the CountVectorizer API (CountVectorizer API file). The CountVectorizer API uses three hyperparameters that can help with overfitting or underfitting while training a subsequent model. The first hyperparameter is max_features which you set to be the vocabulary size. As noted, a very large vocabulary consisting of infrequent words can add unnecessary noise to the data, which will cause you to train a poor model. The second and third hyperparameters are max_df and min_df. The min_df parameter ignores words that occur in less than min_df % documents and max_df ignores words that occur in more than max_df % of the documents. The parameter max_df ensures that extremely frequent words that are not captured by the stop words are removed. Generally, this approach is a good practice as the topic model is trying to identify topics by finding distinct groups of words that cluster together into topics. If a few words occur in all of the documents, these words will reduce the expressiveness of the model. Conversely, increasing the min_df parameter ensures that extremely rare words are not included, which reduces the tendency of the model to overfit.  To generate training and validation sets, you first shuffle the BOW vectors generated by the CountVectorizer API. While performing the shuffle, you keep track of the original index as well as the shuffled index. 


Next it was time to stage the training and validation datasets in S3. I started by converting the vectors to a sparse representation (Convert vectors to sparse file). I then split it into /TrainingData and /TestData. The training data (80%) is used during the model training loop. You use gradient-based optimization to iteratively refine the model parameters. Gradient-based optimization is a way to find model parameter values that minimize the model error, using the gradient of the model loss function. The test data (remaining 20% of customers) is used to evaluate the performance of the model, and measure how well the trained model generalizes to unseen data (Split data file). Next I defined the training and validation paths, as well as the output path where the NTM artifacts will be stored after model training (define training and validation paths). The NTM supports both CSV and RecordIO protobuf formats for data in the training, validation, and testing channel. The following helper function converts the raw vectors into RecordIO format, and using the n_parts parameter, optionally breaks the dataset into shards which can be used for distributed training (Convert vectors to RecordIO format - file ). 



## Train and deploy the topic model
I started by creating and running the training job. The built-in Amazon SageMaker algorithms are stored as docker containers in Amazon Elastic Container Registry (Amazon ECR). For model training, you first need to specify the location of the NTM container in Amazon ECR, closest to your region. The Amazon SageMaker Python SDK includes the /sagemaker.estimator.Estimator estimator. This estimator allows you to specify the infrastructure (Amazon EC2 instance type, number of instances, hyperparameters, output path, and optionally, any security-related settings (virtual private cloud (VPC), security groups, etc.) that may be relevant if we are training our model in a custom VPC of our choice as opposed to an Amazon VPC. The NTM fully takes advantage of GPU hardware and, in general, trains roughly an order of magnitude faster on a GPU than on a CPU. Multi-GPU or multi-instance training further improves training speed roughly linearly if communication overhead is low compared to compute time (sagemaker.estimator.Estimator file). Next it was time to create an instance of the sagemaker.estimator.Estimator class (Sagemaker instance file). Then set the hyperparameters for the topic model (hyperparameters file). Also SageMaker offers two modes for data channels:


- FullyReplicated: All data files are copied to all workers.
- ShardedByS3Key: Data files are sharded to different workers, that is, each worker receives a different portion of the full data set.

At the time of writing, by default, the Amazon SageMaker Python SDK uses FullyReplicated mode for all data channels. This mode is desirable for validation (test) channel but not as efficient for the training channel, when you use multiple workers. In this case, I want to have each worker go through a different portion of the full dataset to provide different gradients within epochs. I specify distribution to be ShardedByS3Key (ShardedByS3Key file). 


It was now time to deploy the topic model, A trained model by itself is simply a tar file consisting of the model weights and does nothing on its own. To make the model useful and get predictions, I needed to deploy the model. There are two ways to deploy the model in Amazon SageMaker, depending on how you want to generate inferences:

Option #1 - To get one inference at a time, set up a persistent endpoint using Amazon SageMaker hosting services.

Option #2 - To get inferences for an entire dataset, use Amazon SageMaker batch transform.
This lab provides both options for you to choose the best approach for your use case.


I decided to go with option #2 because i felt like it would be a little bit quicker. With batch transform, you can run inferences on a batch of data at a time. Amazon SageMaker creates the necessary compute infrastructure and tears it down once the batch job is completed. The batch transform code creates a sagemaker.transformer.Transformer (sagemaker.transformer.Transformer file) object from the topic model. Then, it calls that object's transform method to create a transform job. When you create the sagemaker.transformer.Transformer object, you specify the number and type of instances to use to perform the batch transform job, and the location in Amazon S3 where you want to store the inferences. Once the transform job finished, it was time to download the outputs back to my local notebook instance for inspection (outputs file).

Finally it was time to explore the topic model, the approach I used for exploring the model outputs is to visualize the topic vectors generated using a T-SNE plot. A T-SNE, or t-Distributed Stochastic Neighbor Embedding, is a non-linear technique for dimensionality reduction which aims to ensure that the distance between nearest neighbors in the original high dimensional space is preserved in the resulting lower dimensional space. By setting the number of dimensions to 2, it can be used as an visualization tool to visualize the topic vectors in 2D space (TSNE-plot file). 



## Train and deploy the content recommendation model
Now it was time to create and run the job, before I had created topic vectors. Now it is time to build and deploy the content recommendation model which retains an index of the topic vectors. First, I had to create a dictionary which links the shuffled labels to the original labels in the training data (Create dictionary file). Then it was time to store the training data in the S3 bucket (Store traing data in s3 file). Now, it was time to use a function to create a k-NN estimator much like the NTM estimator I created before (k-NN estimator file). After taking a closer look at the parameters in the helper function, I noticed the Amazon SageMaker k-NN algorithm offers a number of different distance metrics for calculating the nearest neighbors. One popular metric that is used in natural language processing is the cosine distance. Mathematically, the cosine “similarity” between two vectors A and B is given by the following equation:



![image](https://user-images.githubusercontent.com/106786020/215220189-574df133-38c9-4500-b2a9-f3ceaba6b2cf.png)

By setting the index_metric to COSINE, Amazon SageMaker automatically uses the cosine similarity for computing the nearest neighbors. The default distance is the L2 norm, which is the standard Euclidean distance. Note that, at publication, COSINE is only supported for faiss.IVFFlat index type and not the faiss.IVFPQ indexing method.

It was now time to deploy the content recommendation model, As I did with the NTM model, I da to define a helper function for the k-NN model to launch the endpoint. In the helper function, the accept token applications/jsonlines; verbose=true tells the k-NN model to return all the cosine distances instead of just the closest neighbor. To build a recommendation engine, I need to get the top-k suggestions by the model, for which I need to set the verbose parameter to true, instead of the default, false (function for the k-NN model to launch the endpoint file).



